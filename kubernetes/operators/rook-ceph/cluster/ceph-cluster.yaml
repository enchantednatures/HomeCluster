---
# yaml-language-server: $schema=https://kubernetes-schemas.pages.dev/ceph.rook.io/cephcluster_v1.json
apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  cephVersion:
    image: quay.io/ceph/ceph:v19.2.3
    allowUnsupported: false
  dataDirHostPath: /var/lib/rook
  skipUpgradeChecks: false
  continueUpgradeAfterChecksEvenIfNotHealthy: false
  waitTimeoutForHealthyOSDInMinutes: 10
  mon:
    count: 3
    allowMultiplePerNode: false
  mgr:
    count: 2
    allowMultiplePerNode: false
    modules:
      - name: pg_autoscaler
        enabled: true
      - name: rook
        enabled: true
  dashboard:
    enabled: true
    # ssl: true
  monitoring:
    enabled: true
  network:
    connections:
      encryption:
        enabled: false
      compression:
        enabled: false
      requireMsgr2: false
  crashCollector:
    disable: false
  logCollector:
    enabled: true
    periodicity: daily
    maxLogSize: 500M
  cleanupPolicy:
    confirmation: ""
    sanitizeDisks:
      method: quick
      dataSource: zero
      iteration: 1
    allowUninstallWithVolumes: false
  annotations: {}
  labels: {}
  placement:
    all:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: kubernetes.io/arch
                  operator: In
                  values:
                    - amd64
      tolerations:
        - effect: NoSchedule
          operator: Exists
        - effect: NoExecute
          operator: Exists
  resources:
    mgr:
      limits:
        memory: "1Gi"
      requests:
        cpu: "500m"
        memory: "512Mi"
    mon:
      limits:
        memory: "2Gi"
      requests:
        cpu: "1000m"
        memory: "1Gi"
    osd:
      limits:
        memory: "4Gi"
      requests:
        cpu: "1000m"
        memory: "2Gi"
    prepareosd:
      requests:
        cpu: "500m"
        memory: "50Mi"
    mgr-sidecar:
      limits:
        memory: "100Mi"
      requests:
        cpu: "100m"
        memory: "40Mi"
    crashcollector:
      limits:
        memory: "60Mi"
      requests:
        cpu: "100m"
        memory: "60Mi"
    logcollector:
      limits:
        memory: "1Gi"
      requests:
        cpu: "100m"
        memory: "100Mi"
    cleanup:
      limits:
        memory: "1Gi"
      requests:
        cpu: "500m"
        memory: "100Mi"
  removeOSDsIfOutAndSafeToRemove: false
  priorityClassNames:
    mon: system-node-critical
    osd: system-node-critical
    mgr: system-cluster-critical
  storage:
    useAllNodes: true
    useAllDevices: true
    deviceFilter: "^sd[b-z]|^nvme[0-9]"
    config:
      crushRoot: "default"
      metadataDevice: ""
      databaseSizeMB: "1024"
      osdsPerDevice: "1"
  disruptionManagement:
    managePodBudgets: true
    osdMaintenanceTimeout: 30
    pgHealthCheckTimeout: 0
  healthCheck:
    daemonHealth:
      mon:
        disabled: false
        interval: 45s
      osd:
        disabled: false
        interval: 60s
      status:
        disabled: false
        interval: 60s
    livenessProbe:
      mon:
        disabled: false
      mgr:
        disabled: false
      osd:
        disabled: false

  cephBlockPools:
      - name: ceph-blockpool
        spec:
          failureDomain: host
          replicated:
            size: 3
        storageClass:
          enabled: true
          name: ceph-block
          isDefault: true
          reclaimPolicy: Delete
          allowVolumeExpansion: true
          volumeBindingMode: Immediate
          mountOptions: ["discard"]
          parameters:
            imageFormat: "2"
            imageFeatures: layering,fast-diff,object-map,deep-flatten,exclusive-lock
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
            csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
            csi.storage.k8s.io/fstype: ext4

  # NOTE: After disabling the filesystem, the filesystem can be removed with the following commands:
  # ceph fs fail ceph-filesystem && ceph fs rm ceph-filesystem --yes-i-really-mean-it
  cephFileSystems: []

  cephObjectStores:
      - name: ceph-objectstore
        # see https://github.com/rook/rook/blob/master/Documentation/CRDs/Object-Storage/ceph-object-store-crd.md#object-store-settings for available configuration
        spec:
          metadataPool:
            failureDomain: host
            replicated:
              size: 3
              requireSafeReplicaSize: true
            parameters:
              pg_num: "32"
              pgp_num: "32"
            application: rgw
          dataPool:
            failureDomain: host
            erasureCoded:
              dataChunks: 2
              codingChunks: 1
            parameters:
              bulk: "true"
              pg_num: "128"
              pgp_num: "128"
            application: rgw
          preservePoolsOnDelete: true
          gateway:
            port: 80
            # securePort: 443
            # sslCertificateRef: rook-ceph-rgw-cert
            resources:
              limits:
                cpu: "2000m"
                memory: "4Gi"
              requests:
                cpu: "1000m"
                memory: "2Gi"
            instances: 2
            priorityClassName: system-cluster-critical
            placement:
              nodeAffinity:
                requiredDuringSchedulingIgnoredDuringExecution:
                  nodeSelectorTerms:
                    - matchExpressions:
                        - key: kubernetes.io/arch
                          operator: In
                          values:
                            - amd64
              podAntiAffinity:
                preferredDuringSchedulingIgnoredDuringExecution:
                  - weight: 100
                    podAffinityTerm:
                      labelSelector:
                        matchLabels:
                          app: rook-ceph-rgw
                      topologyKey: kubernetes.io/hostname
              tolerations:
                - effect: NoSchedule
                  operator: Exists
                - effect: NoExecute
                  operator: Exists
            # Configure for high availability
            livenessProbe:
              httpGet:
                path: /
                port: http-rgw
                scheme: HTTP
              initialDelaySeconds: 30
              periodSeconds: 30
              timeoutSeconds: 5
              failureThreshold: 3
            readinessProbe:
              httpGet:
                path: /
                port: http-rgw
                scheme: HTTP
              initialDelaySeconds: 10
              periodSeconds: 10
              timeoutSeconds: 5
              failureThreshold: 3
            startupProbe:
              httpGet:
                path: /
                port: http-rgw
                scheme: HTTP
              initialDelaySeconds: 10
              periodSeconds: 10
              timeoutSeconds: 5
              failureThreshold: 18
          # Enable hosting for Istio service mesh integration
          hosting:
            advertiseEndpoint:
              dnsName: rook-ceph-rgw-ceph-objectstore.rook-ceph.svc.cluster.local
              port: 80
              useTls: false
            dnsNames:
              - rgw.${SECRET_DOMAIN}
        storageClass:
          enabled: true
          name: ceph-bucket
          reclaimPolicy: Delete
          volumeBindingMode: "Immediate"
          parameters:
            region: us-east-1
            objectStoreName: ceph-objectstore
            objectStoreNamespace: rook-ceph
        ingress:
          enabled: false
