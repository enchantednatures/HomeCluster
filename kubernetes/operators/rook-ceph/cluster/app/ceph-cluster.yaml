---
# yaml-language-server: $schema=https://kubernetes-schemas.pages.dev/ceph.rook.io/cephcluster_v1.json
apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  cephVersion:
    image: quay.io/ceph/ceph:v19.2.3
    allowUnsupported: false
  dataDirHostPath: /var/lib/rook
  skipUpgradeChecks: false
  continueUpgradeAfterChecksEvenIfNotHealthy: false
  waitTimeoutForHealthyOSDInMinutes: 15
  mon:
    count: 3  # Optimal for 3-node cluster (quorum)
    allowMultiplePerNode: false
  mgr:
    count: 2
    allowMultiplePerNode: false
    modules:
      - name: pg_autoscaler
        enabled: true
      - name: rook
        enabled: true
  dashboard:
    enabled: true
    ssl: true
    port: 8443
  monitoring:
    enabled: true
  network:
    connections:
      encryption:
        enabled: false  # Disable for better performance on fresh setup
      compression:
        enabled: false  # Disable compression for better performance
      requireMsgr2: false  # Allow msgr1 for compatibility
  crashCollector:
    disable: false
  logCollector:
    enabled: true
    periodicity: daily
    maxLogSize: 500M
  cleanupPolicy:
    confirmation: ""  # Empty to allow normal operation
    sanitizeDisks:
      method: quick
      dataSource: zero
      iteration: 1
    allowUninstallWithVolumes: false  # Prevent accidental deletion
  annotations: {}
  labels: {}
  placement:
    all:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: kubernetes.io/arch
                  operator: In
                  values:
                    - amd64
                - key: kubernetes.io/hostname
                  operator: In
                  values:
                    - work-00
                    - work-01
                    - work-02
      tolerations:
        - effect: NoSchedule
          operator: Exists
        - effect: NoExecute
          operator: Exists
        - key: node-role.kubernetes.io/control-plane
          operator: Exists
          effect: NoSchedule
    # Distribute monitors evenly
    mon:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: kubernetes.io/hostname
                  operator: In
                  values:
                    - work-00
                    - work-01
                    - work-02
      tolerations:
        - effect: NoSchedule
          operator: Exists
        - effect: NoExecute
          operator: Exists
      # Ensure each monitor is on a different node
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: DoNotSchedule
          labelSelector:
            matchLabels:
              app: rook-ceph-mon
    mgr:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: kubernetes.io/hostname
                  operator: In
                  values:
                    - work-00
                    - work-01
                    - work-02
      tolerations:
        - effect: NoSchedule
          operator: Exists
        - effect: NoExecute
          operator: Exists
    osd:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: kubernetes.io/hostname
                  operator: In
                  values:
                    - work-00
                    - work-01
                    - work-02
      tolerations:
        - effect: NoSchedule
          operator: Exists
        - effect: NoExecute
          operator: Exists
  resources:
    mgr:
      limits:
        memory: "2Gi"
      requests:
        cpu: "500m"
        memory: "1Gi"
    mon:
      limits:
        memory: "2Gi"
      requests:
        cpu: "1000m"
        memory: "1Gi"
    osd:
      limits:
        memory: "6Gi"  # Increased for better performance
      requests:
        cpu: "1000m"
        memory: "3Gi"  # Increased for better performance
    prepareosd:
      limits:
        memory: "1Gi"
      requests:
        cpu: "500m"
        memory: "512Mi"
    mgr-sidecar:
      limits:
        memory: "100Mi"
      requests:
        cpu: "100m"
        memory: "40Mi"
    crashcollector:
      limits:
        memory: "60Mi"
      requests:
        cpu: "100m"
        memory: "60Mi"
    logcollector:
      limits:
        memory: "1Gi"
      requests:
        cpu: "100m"
        memory: "100Mi"
    cleanup:
      limits:
        memory: "1Gi"
      requests:
        cpu: "500m"
        memory: "100Mi"
  removeOSDsIfOutAndSafeToRemove: false
  priorityClassNames:
    mon: system-node-critical
    osd: system-node-critical
    mgr: system-cluster-critical
  storage:
    useAllNodes: false  # Explicit node selection for better control
    useAllDevices: false  # Explicit device selection for safety
    config:
      crushRoot: "default"
      metadataDevice: ""
      databaseSizeMB: "1024"  # Conservative for stability
      osdsPerDevice: "1"
      encryptedDevice: "false"  # Disable encryption for performance
      # BlueStore configuration optimized for performance
      bluestoreBlockDbSize: "134217728"  # 128MB block DB
      bluestoreBlockWalSize: "134217728"  # 128MB WAL
      bluestoreCompressionMode: "none"  # Disable compression
      bluestoreCacheSizeHdd: "1073741824"  # 1GB cache for HDD
      bluestoreCacheSizeSsd: "2147483648"  # 2GB cache for SSD
    # Define specific nodes and devices for clean deployment
    nodes:
      - name: "work-00"
        devices:
          - name: "/dev/sdb"
        config:
          storeType: bluestore
          metadataDevice: ""
          databaseSizeMB: "1024"
      - name: "work-01"
        devices:
          - name: "/dev/sdb"
        config:
          storeType: bluestore
          metadataDevice: ""
          databaseSizeMB: "1024"
      - name: "work-02"
        devices:
          - name: "/dev/sdb"
        config:
          storeType: bluestore
          metadataDevice: ""
          databaseSizeMB: "1024"

  disruptionManagement:
    managePodBudgets: true
    osdMaintenanceTimeout: 30
    pgHealthCheckTimeout: 0
  healthCheck:
    daemonHealth:
      mon:
        disabled: false
        interval: 45s
      osd:
        disabled: false
        interval: 60s
      status:
        disabled: false
        interval: 60s
    livenessProbe:
      mon:
        disabled: false
      mgr:
        disabled: false
      osd:
        disabled: false
